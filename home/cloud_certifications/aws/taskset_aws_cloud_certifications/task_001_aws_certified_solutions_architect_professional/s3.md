# S3

[Cheat Sheet - AWS S3](https://tutorialsdojo.com/amazon-s3)

[Cheat Sheet - Amazon Glacier](https://tutorialsdojo.com/amazon-sqs)

[Cheat Sheet - s3-pre-signed-urls-vs-cloudfront-signed-urls-vs-origin-access-identity-oai](https://tutorialsdojo.com/s3-pre-signed-urls-vs-cloudfront-signed-urls-vs-origin-access-identity-oai)

[Cheat Sheet - s3-transfer-acceleration-vs-direct-connect-vs-vpn-vs-snowball-vs-snowmobile](https://tutorialsdojo.com/s3-transfer-acceleration-vs-direct-connect-vs-vpn-vs-snowball-vs-snowmobile)

[What is Amazon S3?](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html)

- Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. 

[object-lifecycle-mgmt](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html)

- storing logs in Amazon S3, and use lifecycle policies to archive to Amazon
  Glacier

## Amazon S3 Event Notifications

[Amazon S3 Event Notifications](https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html)

- You can use the Amazon S3 Event Notifications feature to receive notifications when certain events happen in your S3 bucket.
- To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications.
- You store this configuration in the notification subresource that is associated with a bucket.

[Tutorial: Using an Amazon S3 trigger to invoke a Lambda function](https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html#with-s3-example-configure-event-source)


### Replicating objects

[Replicating objects](https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html)

- Replication enables automatic, asynchronous copying of objects across Amazon S3 buckets
- Buckets that are configured for object replication can be owned by the same AWS account or by different accounts.
- Objects may be replicated to a single destination bucket or multiple destination buckets. 
- By default, replication only supports copying new Amazon S3 objects after it is enabled. 
- You can use replication to copy existing objects and clone them to a different bucket, but in order to do so, you must contact AWS Support Center



## Hosting a static website using Amazon S3

[Hosting a static website using Amazon S3](https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html)

- You can use Amazon S3 to host a static website. On a static website, individual webpages include static content. They might also contain client-side scripts.


## Server Side Encryption

### Protecting data using server-side encryption

[Protecting data using server-side encryption](https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html)

- Server-side encryption is the encryption of data at its destination by the application or service that receives it.
- Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. 
- As long as you authenticate your request and you have access permissions, there is no difference in the way you access encrypted or unencrypted objects. 

You have three mutually exclusive options, depending on how you choose to manage the encryption keys.
- Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)
- Server-Side Encryption with KMS keys Stored in AWS Key Management Service (SSE-KMS)
- Server-Side Encryption with Customer-Provided Keys (SSE-C)



### Protecting data using server-side encryption with customer-provided encryption keys (SSE-C)

[Protecting data using server-side encryption with customer-provided encryption keys (SSE-C)](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html)

- Server-side encryption is about protecting data at rest. 
- Server-side encryption encrypts only the object data, not object metadata.
- Using server-side encryption with customer-provided encryption keys (SSE-C) allows you to set your own encryption keys
- The only thing you do is manage the encryption keys you provide.

### Protecting data using server-side encryption with Amazon S3-managed encryption keys (SSE-S3)

[Protecting data using server-side encryption with Amazon S3-managed encryption keys (SSE-S3)](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingServerSideEncryption.html)

- Server-side encryption protects data at rest. 
- Amazon S3 encrypts each object with a unique key. 
- As an additional safeguard, it encrypts the key itself with a key that it rotates regularly. 

## Working with Buckets

### Using Requester Pays buckets for storage transfers and usage

[Using Requester Pays buckets for storage transfers and usage](https://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysBuckets.html)

- In general, bucket owners pay for all Amazon S3 storage and data transfer costs that are associated with their bucket. 
- However, you can configure a bucket to be a Requester Pays bucket. 
- With Requester Pays buckets, the requester instead of the bucket owner pays the cost of the request and the data download from the bucket.


### Using versioning in S3 buckets

[Using versioning in S3 buckets](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html)

- Versioning in Amazon S3 is a means of keeping multiple variants of an object in the same bucket. 
- You can use the S3 Versioning feature to preserve, retrieve, and restore every version of every object stored in your buckets. 

### Configuring fast, secure file transfers using Amazon S3 Transfer Acceleration

[Configuring fast, secure file transfers using Amazon S3 Transfer Acceleration](https://docs.aws.amazon.com/AmazonS3/latest/userguide/transfer-acceleration.html)

- Amazon S3 Transfer Acceleration is a bucket-level feature that enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. 
- Transfer Acceleration takes advantage of the globally distributed edge locations in Amazon CloudFront. 
- As the data arrives at an edge location, the data is routed to Amazon S3 over an optimized network path.

**Why use Transfer Acceleration?**
- Your customers upload to a centralized bucket from all over the world.
- You transfer gigabytes to terabytes of data on a regular basis across continents.
- You can't use all of your available bandwidth over the internet when uploading to Amazon S3.


### Uploading and copying objects using multipart upload

[Uploading and copying objects using multipart upload](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html)

[How can I optimize performance when I upload large files to Amazon S3?](https://aws.amazon.com/premiumsupport/knowledge-center/s3-upload-large-files)

- Multipart upload allows you to upload a single object as a set of parts. 
- Each part is a contiguous portion of the object's data. 
- You can upload these object parts independently and in any order
- If transmission of any part fails, you can retransmit that part without affecting other parts. 
- After all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object. 
- In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation.

## Uploading objects using presigned URLs

[Uploading objects using presigned URLs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/PresignedUrlUploadObject.html)


- A presigned URL gives you access to the object identified in the URL, provided that the creator of the presigned URL has permissions to access that object.
- All objects and buckets by default are private. 
- The presigned URLs are useful if you want your user/customer to be able to upload a specific object to your bucket, but you don't require them to have AWS security credentials or permissions.

## Managing your storage lifecycle

[Managing your storage lifecycle](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html)


## Amazon S3 Glacier storage classes

[Amazon S3 Glacier storage classes](https://aws.amazon.com/s3/storage-classes/glacier)

- The Amazon S3 Glacier storage classes are purpose-built for data archiving, providing you with the highest performance, most retrieval flexibility, and the lowest cost archive storage in the cloud

## Notes

[I accidentally denied everyone access to my Amazon S3 bucket. How do I regain access?](https://aws.amazon.com/premiumsupport/knowledge-center/s3-accidentally-denied-access/)

- To get access to your bucket again, sign in to the Amazon S3 console as the AWS account root user, and then delete the bucket policy.


#### put-bucket-policy

[put-bucket-policy](https://docs.aws.amazon.com/cli/latest/reference/s3api/put-bucket-policy.html)

- This example allows all users to retrieve any object in MyBucket except those in the MySecretFolder. It also grants put and delete permission to the root user of the AWS account 1234-5678-9012

```bash
aws s3api put-bucket-policy --bucket MyBucket --policy file://policy.json
```


```json
{
   "Statement": [
      {
         "Effect": "Allow",
         "Principal": "*",
         "Action": "s3:GetObject",
         "Resource": "arn:aws:s3:::MyBucket/*"
      },
      {
         "Effect": "Deny",
         "Principal": "*",
         "Action": "s3:GetObject",
         "Resource": "arn:aws:s3:::MyBucket/MySecretFolder/*"
      },
      {
         "Effect": "Allow",
         "Principal": {
            "AWS": "arn:aws:iam::123456789012:root"
         },
         "Action": [
            "s3:DeleteObject",
            "s3:PutObject"
         ],
         "Resource": "arn:aws:s3:::MyBucket/*"
      }
   ]
}
```

### Managing Storage

#### 

[How can I retrieve an Amazon S3 object that was deleted in a versioning-enabled bucket?](https://aws.amazon.com/premiumsupport/knowledge-center/s3-undelete-configuration/)

- When you delete an object from a version-enabled bucket, Amazon S3 creates a delete marker for the object. The delete marker becomes the current version of the object, and the actual object becomes the previous version. With a delete marker, Amazon S3 responds to requests for the object as though the object was deleted. For example, if you send a GET request for the object, then Amazon S3 returns an error.


## Blogs

- [Building and Maintaining an Amazon S3 Metadata Index without Servers](https://aws.amazon.com/blogs/big-data/building-and-maintaining-an-amazon-s3-metadata-index-without-servers)

  - Amazon S3 is a simple key-based object store whose scalability and low cost make it ideal for storing large datasets
  - Its design enables S3 to provide excellent performance for storing and retrieving objects based on a known key.
  - Finding objects based on other attributes, however, requires doing a linear search using the LIST operation. 
  - Because each listing can return at most 1000 keys, it may require many requests before finding the object. 
  - Because of these additional requests, implementing attribute-based queries in S3 alone can be challenging.
  - A common solution is to build an external index that maps queryable attributes to the S3 object key
  - approach for building such an index using Amazon DynamoDB and AWS Lambda